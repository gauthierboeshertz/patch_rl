Sender: LSF System <lsfadmin@eu-g3-039>
Subject: Job 235066417: <python3 -m scripts.train_patch_model device=cuda> in cluster <euler> Exited

Job <python3 -m scripts.train_patch_model device=cuda> was submitted from host <eu-login-31> by user <gboeshertz> in cluster <euler> at Sun Nov  6 12:36:54 2022
Job was executed on host(s) <5*eu-g3-039>, in queue <gpu.24h>, as user <gboeshertz> in cluster <euler> at Sun Nov  6 12:37:15 2022
</cluster/home/gboeshertz> was used as the home directory.
</cluster/home/gboeshertz/patch_rl> was used as the working directory.
Started at Sun Nov  6 12:37:15 2022
Terminated at Sun Nov  6 15:14:01 2022
Results reported at Sun Nov  6 15:14:01 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python3 -m scripts.train_patch_model device=cuda
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   11299.77 sec.
    Max Memory :                                 41550 MB
    Average Memory :                             21898.92 MB
    Total Requested Memory :                     50000.00 MB
    Delta Memory :                               8450.00 MB
    Max Swap :                                   -
    Max Processes :                              6
    Max Threads :                                283
    Run time :                                   9434 sec.
    Turnaround time :                            9427 sec.

The output (if any) follows:

Training with config: {'encoder_decoder': {'name': 'patch_vae', '_target_': 'src.patch_vae.PatchVAE', 'patch_size': 16, 'in_channels': 9, 'latent_dim': 256, 'channels': [64, 128, 256], 'kernel_sizes': [3, 3, 3], 'paddings': [1, 1, 1], 'strides': [2, 2, 2], 'kld_weight': 0.001, 'loss_type': 'B', 'gamma': 10.0, 'max_capacity': 25, 'Capacity_max_iter': 10000, 'beta': 4}, 'dynamics': {'_target_': 'src.dynamics_model.AttentionDynamicsModel', 'num_heads': 4, 'mlp_dim': 256, 'num_attention_layers': 4, 'dropout': 0.2, 'action_dim': -1, 'in_features': -1, 'num_patches': -1, 'group_actions': False, 'residual': True, 'use_attn_mask': True, 'discrete_actions': True, 'regularizer_weight': 0}, 'inverse': {'_target_': 'src.inverse_dynamics_model.PatchInverseDynamicsModel', 'encoder': {'in_channels': -1, 'channels': [128, 64, 32, 16], 'kernel_sizes': [3, 3, 3, 3], 'strides': [1, 1, 1, 1], 'paddings': [1, 1, 1, 1], 'dropout': 0.2, 'norm_type': 'bn', 'use_maxpool': False}, 'mlp': {'input_size': -1, 'layer_sizes': [256, 128, 64], 'output_size': -1}}, 'device': 'cuda', 'loss_weights': {'dyn_loss_weight': 0, 'vae_loss_weight': 1, 'inverse_loss_weight': 0}, 'env': {'num_transitions': 10000, 'num_sprites': 4, 'all_sprite_mover': False, 'one_sprite_mover': False, 'discrete_all_sprite_mover': True, 'random_init_places': True, 'num_action_repeat': 4, 'instant_move': False}, 'train_loop': {'batch_size': 8, 'num_epochs': 200, 'num_workers': 3, 'lr': 0.0001}}
torch.Size([10000, 1, 128, 128, 3])
The encoder ouputs 64 patches
The encoder ouputs 256 features per patch
{'_target_': 'src.inverse_dynamics_model.PatchInverseDynamicsModel', 'encoder': {'in_channels': 512, 'channels': [128, 64, 32, 16], 'kernel_sizes': [3, 3, 3, 3], 'strides': [1, 1, 1, 1], 'paddings': [1, 1, 1, 1], 'dropout': 0.2, 'norm_type': 'bn', 'use_maxpool': False}, 'mlp': {'input_size': 1024, 'layer_sizes': [256, 128, 64], 'output_size': 16}}
{'in_channels': 512, 'channels': [128, 64, 32, 16], 'kernel_sizes': [3, 3, 3, 3], 'strides': [1, 1, 1, 1], 'paddings': [1, 1, 1, 1], 'dropout': 0.2, 'norm_type': 'bn', 'use_maxpool': False}
Creating MLP with input size 1024 and output size 16
Creating MLP with layer sizes [256, 128, 64]
[2022-11-06 12:39:57,124][__main__][INFO] - Epoch: 0, TRAIN: vae_loss: 0.07237, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.05026, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 12:42:12,146][__main__][INFO] - Epoch: 1, TRAIN: vae_loss: 0.04280, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.03766, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 12:44:27,207][__main__][INFO] - Epoch: 2, TRAIN: vae_loss: 0.02729, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.02349, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 12:46:41,905][__main__][INFO] - Epoch: 3, TRAIN: vae_loss: 0.01922, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.01739, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 12:48:57,364][__main__][INFO] - Epoch: 4, TRAIN: vae_loss: 0.01509, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.01621, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 12:51:11,348][__main__][INFO] - Epoch: 5, TRAIN: vae_loss: 0.01276, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.01233, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 12:53:27,400][__main__][INFO] - Epoch: 6, TRAIN: vae_loss: 0.01053, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.01187, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 12:55:42,782][__main__][INFO] - Epoch: 7, TRAIN: vae_loss: 0.00919, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.01189, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 12:57:57,315][__main__][INFO] - Epoch: 8, TRAIN: vae_loss: 0.00847, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00810, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:00:13,020][__main__][INFO] - Epoch: 9, TRAIN: vae_loss: 0.00749, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00742, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:02:27,841][__main__][INFO] - Epoch: 10, TRAIN: vae_loss: 0.00680, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00712, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:04:43,477][__main__][INFO] - Epoch: 11, TRAIN: vae_loss: 0.00634, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00611, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:06:59,170][__main__][INFO] - Epoch: 12, TRAIN: vae_loss: 0.00589, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00624, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:09:14,454][__main__][INFO] - Epoch: 13, TRAIN: vae_loss: 0.00541, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00608, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:11:29,867][__main__][INFO] - Epoch: 14, TRAIN: vae_loss: 0.00529, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00546, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:13:44,685][__main__][INFO] - Epoch: 15, TRAIN: vae_loss: 0.00491, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00528, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:15:59,105][__main__][INFO] - Epoch: 16, TRAIN: vae_loss: 0.00472, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00548, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:18:13,540][__main__][INFO] - Epoch: 17, TRAIN: vae_loss: 0.00462, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00591, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:20:29,075][__main__][INFO] - Epoch: 18, TRAIN: vae_loss: 0.00430, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00567, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:22:43,784][__main__][INFO] - Epoch: 19, TRAIN: vae_loss: 0.00426, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00474, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:24:59,985][__main__][INFO] - Epoch: 20, TRAIN: vae_loss: 0.00411, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00445, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:27:15,931][__main__][INFO] - Epoch: 21, TRAIN: vae_loss: 0.00394, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00439, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:29:32,458][__main__][INFO] - Epoch: 22, TRAIN: vae_loss: 0.00391, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00450, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:31:48,393][__main__][INFO] - Epoch: 23, TRAIN: vae_loss: 0.00369, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00468, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:34:03,963][__main__][INFO] - Epoch: 24, TRAIN: vae_loss: 0.00353, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00398, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:36:19,547][__main__][INFO] - Epoch: 25, TRAIN: vae_loss: 0.00347, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00406, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:38:34,773][__main__][INFO] - Epoch: 26, TRAIN: vae_loss: 0.00355, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00410, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:40:50,924][__main__][INFO] - Epoch: 27, TRAIN: vae_loss: 0.00333, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00395, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:43:07,608][__main__][INFO] - Epoch: 28, TRAIN: vae_loss: 0.00315, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00393, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:45:23,964][__main__][INFO] - Epoch: 29, TRAIN: vae_loss: 0.00317, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00463, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:47:39,391][__main__][INFO] - Epoch: 30, TRAIN: vae_loss: 0.00312, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00359, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:49:55,038][__main__][INFO] - Epoch: 31, TRAIN: vae_loss: 0.00302, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00358, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:52:10,655][__main__][INFO] - Epoch: 32, TRAIN: vae_loss: 0.00303, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00378, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:54:25,815][__main__][INFO] - Epoch: 33, TRAIN: vae_loss: 0.00295, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00354, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:56:41,539][__main__][INFO] - Epoch: 34, TRAIN: vae_loss: 0.00282, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00361, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 13:58:57,615][__main__][INFO] - Epoch: 35, TRAIN: vae_loss: 0.00280, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00350, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:01:13,981][__main__][INFO] - Epoch: 36, TRAIN: vae_loss: 0.00261, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00364, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:03:29,218][__main__][INFO] - Epoch: 37, TRAIN: vae_loss: 0.00265, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00389, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:05:44,397][__main__][INFO] - Epoch: 38, TRAIN: vae_loss: 0.00261, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00362, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:07:59,484][__main__][INFO] - Epoch: 39, TRAIN: vae_loss: 0.00263, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00354, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:10:15,575][__main__][INFO] - Epoch: 40, TRAIN: vae_loss: 0.00267, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00333, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:12:31,542][__main__][INFO] - Epoch: 41, TRAIN: vae_loss: 0.00256, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00352, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:14:47,747][__main__][INFO] - Epoch: 42, TRAIN: vae_loss: 0.00253, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00374, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:17:03,808][__main__][INFO] - Epoch: 43, TRAIN: vae_loss: 0.00249, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00312, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:19:20,344][__main__][INFO] - Epoch: 44, TRAIN: vae_loss: 0.00234, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00318, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:21:36,629][__main__][INFO] - Epoch: 45, TRAIN: vae_loss: 0.00245, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00316, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:23:52,628][__main__][INFO] - Epoch: 46, TRAIN: vae_loss: 0.00230, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00326, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:26:07,896][__main__][INFO] - Epoch: 47, TRAIN: vae_loss: 0.00237, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00397, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:28:23,344][__main__][INFO] - Epoch: 48, TRAIN: vae_loss: 0.00245, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00298, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:30:38,639][__main__][INFO] - Epoch: 49, TRAIN: vae_loss: 0.00221, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00318, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:32:54,443][__main__][INFO] - Epoch: 50, TRAIN: vae_loss: 0.00229, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00333, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:35:09,082][__main__][INFO] - Epoch: 51, TRAIN: vae_loss: 0.00219, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00314, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:37:24,320][__main__][INFO] - Epoch: 52, TRAIN: vae_loss: 0.00211, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00325, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:39:38,618][__main__][INFO] - Epoch: 53, TRAIN: vae_loss: 0.00224, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00328, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:41:53,902][__main__][INFO] - Epoch: 54, TRAIN: vae_loss: 0.00207, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00310, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:44:09,915][__main__][INFO] - Epoch: 55, TRAIN: vae_loss: 0.00215, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00294, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:46:25,482][__main__][INFO] - Epoch: 56, TRAIN: vae_loss: 0.00210, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00298, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:48:40,343][__main__][INFO] - Epoch: 57, TRAIN: vae_loss: 0.00214, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00381, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:50:56,483][__main__][INFO] - Epoch: 58, TRAIN: vae_loss: 0.00193, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00283, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:53:12,274][__main__][INFO] - Epoch: 59, TRAIN: vae_loss: 0.00195, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00280, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:55:28,228][__main__][INFO] - Epoch: 60, TRAIN: vae_loss: 0.00199, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00278, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:57:44,034][__main__][INFO] - Epoch: 61, TRAIN: vae_loss: 0.00207, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00351, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 14:59:59,369][__main__][INFO] - Epoch: 62, TRAIN: vae_loss: 0.00199, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00349, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 15:02:15,548][__main__][INFO] - Epoch: 63, TRAIN: vae_loss: 0.00193, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00285, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 15:04:30,661][__main__][INFO] - Epoch: 64, TRAIN: vae_loss: 0.00198, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00275, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 15:06:46,205][__main__][INFO] - Epoch: 65, TRAIN: vae_loss: 0.00190, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00284, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 15:09:01,753][__main__][INFO] - Epoch: 66, TRAIN: vae_loss: 0.00184, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00309, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 15:11:16,779][__main__][INFO] - Epoch: 67, TRAIN: vae_loss: 0.00189, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00264, dyn_loss: 0.00000, inverse_loss: 0.00000, 
[2022-11-06 15:13:32,112][__main__][INFO] - Epoch: 68, TRAIN: vae_loss: 0.00186, dyn_loss: 0.00000, inverse_loss: 0.00000, VAL: vae_loss: 0.00282, dyn_loss: 0.00000, inverse_loss: 0.00000, 
Traceback (most recent call last):
  File "/cluster/home/gboeshertz/miniconda3/envs/urlb/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/home/gboeshertz/miniconda3/envs/urlb/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/home/gboeshertz/patch_rl/scripts/train_patch_model.py", line 149, in <module>
    
  File "/cluster/home/gboeshertz/miniconda3/envs/urlb/lib/python3.8/site-packages/hydra/main.py", line 49, in decorated_main
    _run_hydra(
  File "/cluster/home/gboeshertz/miniconda3/envs/urlb/lib/python3.8/site-packages/hydra/_internal/utils.py", line 367, in _run_hydra
    run_and_report(
  File "/cluster/home/gboeshertz/miniconda3/envs/urlb/lib/python3.8/site-packages/hydra/_internal/utils.py", line 211, in run_and_report
    return func()
  File "/cluster/home/gboeshertz/miniconda3/envs/urlb/lib/python3.8/site-packages/hydra/_internal/utils.py", line 368, in <lambda>
    lambda: hydra.run(
  File "/cluster/home/gboeshertz/miniconda3/envs/urlb/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 97, in run
    ret = run_job(
  File "/cluster/home/gboeshertz/miniconda3/envs/urlb/lib/python3.8/site-packages/hydra/core/utils.py", line 160, in run_job
    ret.return_value = task_function(task_cfg)
  File "/cluster/home/gboeshertz/patch_rl/scripts/train_patch_model.py", line 143, in main
    print("Training Done")
  File "/cluster/home/gboeshertz/patch_rl/scripts/train_patch_model.py", line 67, in train
    train_loss, train_loss_dict = train_epoch(model,optimizer, train_dataloader)
  File "/cluster/home/gboeshertz/patch_rl/scripts/train_patch_model.py", line 36, in train_epoch
    epoch_loss_dict[k] += float(loss_dict[k])
KeyboardInterrupt
